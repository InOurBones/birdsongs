{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"text-align:center\">Birdsong Classification song</h1>\n",
    "\n",
    "In this notebook I'll train a classifier which can predict the species of a bird based on new audio.\n",
    "For this notebook I've limited to the specifies which start with an 'a' and 'b' to reduce the load and training time.\n",
    "Eventually the dataset could be extended to use the other datasets as well, but that won't be done in this notebook.\n",
    "the dataset used for this can be found on: [https://www.kaggle.com/ttahara/birdsong-resampled-train-audio-00](https://www.kaggle.com/ttahara/birdsong-resampled-train-audio-00)\n",
    "\n",
    "It is common knowledge that raw audio files cannot be used as input for a ML model. They have to be transformed first into something the computer understands. Therefore, an audio can be transformed into a spectogram. This visualizes the strength of a soundwave over the time.\n",
    "\n",
    "There are multiple libraries to convert audio files to spectograms such as scipy, torch, pylab and librosa to name a few. At my job as data engineer/analysist I've already created ML audio to identify music genres of audio tracks. For that we used scipy. To spice things up I've choosen to use PyTorch to experience something new.\n",
    "\n",
    "the different steps for this notebook are described below:\n",
    "<ol>\n",
    "    <li>sound files</li>\n",
    "    <li>convert into spectrograms</li>\n",
    "    <li>input into CNN plus Linear Classifier model</li>\n",
    "    <li>produce predictions and evaluate model</li>\n",
    "</ol>\n",
    "<img src=\"https://i.imgur.com/JZ6JZWg.png\" alt=\"Audio Classification Application\" />\n",
    "\n",
    "For a future experiment the dataset can be cleaned up by removing the background noise from the dataset. If I have time left I might do it in here as well, but as of now it's outside of my scope."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# step 1: metadata\n",
    "The first step is to read the metadata and load in into a pd.DataFrame.\n",
    "In our case there already is a metadata file available. If this wasn't the case, we would've had to check for every possible path and get our information from there.\n",
    "\n",
    "In the metadata file there is a lot of information which is not needed for training the model.\n",
    "To clean it up we will load the file into a DataFrame and then turn it in such a way that we only have 2 columns left: bird species and relative path to the audio file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean duration: 59.16556145004421\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rating</th>\n",
       "      <th>playback_used</th>\n",
       "      <th>ebird_code</th>\n",
       "      <th>channels</th>\n",
       "      <th>date</th>\n",
       "      <th>pitch</th>\n",
       "      <th>duration</th>\n",
       "      <th>filename</th>\n",
       "      <th>speed</th>\n",
       "      <th>species</th>\n",
       "      <th>...</th>\n",
       "      <th>author</th>\n",
       "      <th>primary_label</th>\n",
       "      <th>longitude</th>\n",
       "      <th>length</th>\n",
       "      <th>time</th>\n",
       "      <th>recordist</th>\n",
       "      <th>license</th>\n",
       "      <th>resampled_sampling_rate</th>\n",
       "      <th>resampled_filename</th>\n",
       "      <th>resampled_channels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.5</td>\n",
       "      <td>no</td>\n",
       "      <td>aldfly</td>\n",
       "      <td>1 (mono)</td>\n",
       "      <td>2013-05-25</td>\n",
       "      <td>Not specified</td>\n",
       "      <td>25</td>\n",
       "      <td>XC134874.mp3</td>\n",
       "      <td>Not specified</td>\n",
       "      <td>Alder Flycatcher</td>\n",
       "      <td>...</td>\n",
       "      <td>Jonathon Jongsma</td>\n",
       "      <td>Empidonax alnorum_Alder Flycatcher</td>\n",
       "      <td>-92.962</td>\n",
       "      <td>Not specified</td>\n",
       "      <td>8:00</td>\n",
       "      <td>Jonathon Jongsma</td>\n",
       "      <td>Creative Commons Attribution-ShareAlike 3.0</td>\n",
       "      <td>32000</td>\n",
       "      <td>XC134874.wav</td>\n",
       "      <td>1 (mono)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.0</td>\n",
       "      <td>no</td>\n",
       "      <td>aldfly</td>\n",
       "      <td>2 (stereo)</td>\n",
       "      <td>2013-05-27</td>\n",
       "      <td>both</td>\n",
       "      <td>36</td>\n",
       "      <td>XC135454.mp3</td>\n",
       "      <td>both</td>\n",
       "      <td>Alder Flycatcher</td>\n",
       "      <td>...</td>\n",
       "      <td>Mike Nelson</td>\n",
       "      <td>Empidonax alnorum_Alder Flycatcher</td>\n",
       "      <td>-82.1106</td>\n",
       "      <td>0-3(s)</td>\n",
       "      <td>08:30</td>\n",
       "      <td>Mike Nelson</td>\n",
       "      <td>Creative Commons Attribution-NonCommercial-Sha...</td>\n",
       "      <td>32000</td>\n",
       "      <td>XC135454.wav</td>\n",
       "      <td>1 (mono)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.0</td>\n",
       "      <td>no</td>\n",
       "      <td>aldfly</td>\n",
       "      <td>2 (stereo)</td>\n",
       "      <td>2013-05-27</td>\n",
       "      <td>both</td>\n",
       "      <td>39</td>\n",
       "      <td>XC135455.mp3</td>\n",
       "      <td>both</td>\n",
       "      <td>Alder Flycatcher</td>\n",
       "      <td>...</td>\n",
       "      <td>Mike Nelson</td>\n",
       "      <td>Empidonax alnorum_Alder Flycatcher</td>\n",
       "      <td>-82.1106</td>\n",
       "      <td>0-3(s)</td>\n",
       "      <td>08:30</td>\n",
       "      <td>Mike Nelson</td>\n",
       "      <td>Creative Commons Attribution-NonCommercial-Sha...</td>\n",
       "      <td>32000</td>\n",
       "      <td>XC135455.wav</td>\n",
       "      <td>1 (mono)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.5</td>\n",
       "      <td>no</td>\n",
       "      <td>aldfly</td>\n",
       "      <td>2 (stereo)</td>\n",
       "      <td>2013-05-27</td>\n",
       "      <td>both</td>\n",
       "      <td>33</td>\n",
       "      <td>XC135456.mp3</td>\n",
       "      <td>both</td>\n",
       "      <td>Alder Flycatcher</td>\n",
       "      <td>...</td>\n",
       "      <td>Mike Nelson</td>\n",
       "      <td>Empidonax alnorum_Alder Flycatcher</td>\n",
       "      <td>-82.1106</td>\n",
       "      <td>0-3(s)</td>\n",
       "      <td>08:30</td>\n",
       "      <td>Mike Nelson</td>\n",
       "      <td>Creative Commons Attribution-NonCommercial-Sha...</td>\n",
       "      <td>32000</td>\n",
       "      <td>XC135456.wav</td>\n",
       "      <td>1 (mono)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.0</td>\n",
       "      <td>no</td>\n",
       "      <td>aldfly</td>\n",
       "      <td>2 (stereo)</td>\n",
       "      <td>2013-05-27</td>\n",
       "      <td>both</td>\n",
       "      <td>36</td>\n",
       "      <td>XC135457.mp3</td>\n",
       "      <td>level</td>\n",
       "      <td>Alder Flycatcher</td>\n",
       "      <td>...</td>\n",
       "      <td>Mike Nelson</td>\n",
       "      <td>Empidonax alnorum_Alder Flycatcher</td>\n",
       "      <td>-82.1106</td>\n",
       "      <td>0-3(s)</td>\n",
       "      <td>08:30</td>\n",
       "      <td>Mike Nelson</td>\n",
       "      <td>Creative Commons Attribution-NonCommercial-Sha...</td>\n",
       "      <td>32000</td>\n",
       "      <td>XC135457.wav</td>\n",
       "      <td>1 (mono)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 38 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   rating playback_used ebird_code    channels        date          pitch  \\\n",
       "0     3.5            no     aldfly    1 (mono)  2013-05-25  Not specified   \n",
       "1     4.0            no     aldfly  2 (stereo)  2013-05-27           both   \n",
       "2     4.0            no     aldfly  2 (stereo)  2013-05-27           both   \n",
       "3     3.5            no     aldfly  2 (stereo)  2013-05-27           both   \n",
       "4     4.0            no     aldfly  2 (stereo)  2013-05-27           both   \n",
       "\n",
       "   duration      filename          speed           species  ...  \\\n",
       "0        25  XC134874.mp3  Not specified  Alder Flycatcher  ...   \n",
       "1        36  XC135454.mp3           both  Alder Flycatcher  ...   \n",
       "2        39  XC135455.mp3           both  Alder Flycatcher  ...   \n",
       "3        33  XC135456.mp3           both  Alder Flycatcher  ...   \n",
       "4        36  XC135457.mp3          level  Alder Flycatcher  ...   \n",
       "\n",
       "             author                       primary_label longitude  \\\n",
       "0  Jonathon Jongsma  Empidonax alnorum_Alder Flycatcher   -92.962   \n",
       "1       Mike Nelson  Empidonax alnorum_Alder Flycatcher  -82.1106   \n",
       "2       Mike Nelson  Empidonax alnorum_Alder Flycatcher  -82.1106   \n",
       "3       Mike Nelson  Empidonax alnorum_Alder Flycatcher  -82.1106   \n",
       "4       Mike Nelson  Empidonax alnorum_Alder Flycatcher  -82.1106   \n",
       "\n",
       "          length   time         recordist  \\\n",
       "0  Not specified   8:00  Jonathon Jongsma   \n",
       "1         0-3(s)  08:30       Mike Nelson   \n",
       "2         0-3(s)  08:30       Mike Nelson   \n",
       "3         0-3(s)  08:30       Mike Nelson   \n",
       "4         0-3(s)  08:30       Mike Nelson   \n",
       "\n",
       "                                             license resampled_sampling_rate  \\\n",
       "0        Creative Commons Attribution-ShareAlike 3.0                   32000   \n",
       "1  Creative Commons Attribution-NonCommercial-Sha...                   32000   \n",
       "2  Creative Commons Attribution-NonCommercial-Sha...                   32000   \n",
       "3  Creative Commons Attribution-NonCommercial-Sha...                   32000   \n",
       "4  Creative Commons Attribution-NonCommercial-Sha...                   32000   \n",
       "\n",
       "  resampled_filename resampled_channels  \n",
       "0       XC134874.wav           1 (mono)  \n",
       "1       XC135454.wav           1 (mono)  \n",
       "2       XC135455.wav           1 (mono)  \n",
       "3       XC135456.wav           1 (mono)  \n",
       "4       XC135457.wav           1 (mono)  \n",
       "\n",
       "[5 rows x 38 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "download_path = Path.cwd()/'birdsongs'\n",
    "metadata_file = download_path/'train_mod.csv'\n",
    "df = pd.read_csv(metadata_file)\n",
    "df = df[df.ebird_code.astype(str).str.startswith('a') | df.ebird_code.astype(str).str.startswith('b')]\n",
    "print(f\"Mean duration: {df.duration.mean()}\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ebird_code</th>\n",
       "      <th>relative_path</th>\n",
       "      <th>classID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>aldfly</td>\n",
       "      <td>/aldfly/XC134874.wav</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>aldfly</td>\n",
       "      <td>/aldfly/XC135454.wav</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>aldfly</td>\n",
       "      <td>/aldfly/XC135455.wav</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>aldfly</td>\n",
       "      <td>/aldfly/XC135456.wav</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>aldfly</td>\n",
       "      <td>/aldfly/XC135457.wav</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  ebird_code         relative_path  classID\n",
       "0     aldfly  /aldfly/XC134874.wav        0\n",
       "1     aldfly  /aldfly/XC135454.wav        0\n",
       "2     aldfly  /aldfly/XC135455.wav        0\n",
       "3     aldfly  /aldfly/XC135456.wav        0\n",
       "4     aldfly  /aldfly/XC135457.wav        0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['relative_path'] = '/' + df['ebird_code'].astype(str) + \"/\" + df['resampled_filename'].astype(str)\n",
    "\n",
    "# using f\"/{df['ebird_code']}/{df['resampled_filename']}\" results in an error :(\n",
    "\n",
    "df = df[['ebird_code', 'relative_path']]\n",
    "df = df[df.ebird_code.astype(str).str.startswith('a') | df.ebird_code.astype(str).str.startswith('b')]\n",
    "\n",
    "bird_ids = pd.DataFrame(df.ebird_code.unique()).set_index(0)\n",
    "df['classID'] = df['ebird_code'].apply(lambda x: bird_ids.index.get_loc(x))\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# step 2: convert audio to a spectogram\n",
    "\n",
    "For a dataset to be useable, the datatypes of the different columns must be the same. For audio, this is a bit different. As mentioned at the start, I've done this before using scipy. I'll use that knowledge to recreate the class, but using PyTorch.\n",
    "\n",
    "From personal experience, the most common difference I encounter are:\n",
    "<ul>\n",
    "    <li>mono vs stero (number of channels)</li>\n",
    "    <li>sample rates</li>\n",
    "    <li>audio length</li>\n",
    "</ul>\n",
    "\n",
    "From the original metadata file mentioned in the previous step, I noticed that all these 3 problems also occur in this dataset.\n",
    "\n",
    "* Both mono and stereo appears in the datset.\n",
    "* The sampling rate ranges from 8000Hz to 11025Hz. \n",
    "* the audio length ranges from 1s to 2283s (38 min).\n",
    "\n",
    "\n",
    "In here we also create a Mel Spectogram. For me the 'Mel' version is new, but my CTO (who has a PHD in ML with Music) told me to look into it as this might contribute to both the model and learn me something new. My understanding of the 'Mel' is as follows:\n",
    "\n",
    "Mel is a different scale compared to the regular spectogram which plots frequency on the time. Mel spectogram uses the Mel scale instead of frequency. There is a whole mathematical reason behind this, one which is a bit too complicated for me, but the take away is that it's better than just frequency.\n",
    "for more information I suggest to read [https://medium.com/analytics-vidhya/understanding-the-mel-spectrogram-fca2afa2ce53](https://medium.com/analytics-vidhya/understanding-the-mel-spectrogram-fca2afa2ce53)\n",
    "\n",
    "Secondly, we will make a PyTorch Dataset Object which will use all the steps load and modify the audio, which then can be used for training our model. The duration will be changed to 59 since this is the mean() of the duration from our dataset.\n",
    "\n",
    "@TODO: I'm currently still exploring these numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math, random\n",
    "import torch\n",
    "import torchaudio\n",
    "from torchaudio import transforms\n",
    "from IPython.display import Audio\n",
    "from torch.utils.data import DataLoader, Dataset, random_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioUtil():\n",
    "    \"\"\"\n",
    "    This class contains all the methods needed for the pre-processing transform steps\n",
    "    \"\"\"\n",
    "    @staticmethod\n",
    "    def open(audio_file):\n",
    "        \"\"\"\n",
    "        Load the audio and return the signal\n",
    "        :param audio_file:\n",
    "        :type audio_file:\n",
    "        :return: returns a tuple of the signal and sample rate\n",
    "        \"\"\"\n",
    "        sig, sr = torchaudio.load(audio_file)\n",
    "        return (sig, sr)\n",
    "    \n",
    "    @staticmethod\n",
    "    def rechannel(aud, new_channel):\n",
    "        \"\"\"\n",
    "        Audio can be mono or stereo, this has to be uniform.\n",
    "        Therefore, we convert any mono (1 audio channel) to stereo (2 audio channels) or vice verse depending on the new_channel input.\n",
    "        :param aud: tuple of signal and sample rate\n",
    "        :type aud: tuple\n",
    "        :param new_channel: mono = 1 or stereo = 2\n",
    "        :type new_channel: int\n",
    "        :return: returns a tuple of a tuple with the signal and sample rate\n",
    "        \"\"\"\n",
    "        sig, sr = aud\n",
    "        if (sig.shape[0] == new_channel): # if its already the desired channels, do nothing\n",
    "            return aud\n",
    "        \n",
    "        if (new_channel == 1):\n",
    "            resig = sig[:1, :] # convert stereo to mono by selecting the 1st channel\n",
    "        else:\n",
    "            resig = torch.cat([sig, sig]) # convert mono to stero by duplicating the 1st channel\n",
    "        \n",
    "        return ((resig, sr))\n",
    "    \n",
    "    @staticmethod\n",
    "    def resample(aud, newsr):\n",
    "        \"\"\"\n",
    "        Audio can have multiple sample rates, this has to be uniform.\n",
    "        Therefore, we convert the audio to the newsr from the input using torchaudio.transforms\n",
    "        :param aud: tuple of signal and sample rate\n",
    "        :type aud: tuple\n",
    "        :param newsr: the desired sample rate\n",
    "        :type newsr: int\n",
    "        :return: returns a tuple of a tuple with the signal and sample rate\n",
    "        \"\"\"\n",
    "        sig, sr = aud\n",
    "        \n",
    "        if (sr == newsr): # if its already the desired sample rate, do nothing\n",
    "            return aud\n",
    "        \n",
    "        num_channels = sig.shape[0]\n",
    "        \n",
    "        resig = torchaudio.transforms.Resample(sr, newsr)(sig[:1,:]) # resample the 1st channel\n",
    "        if (num_channels > 1): # if audio is stero\n",
    "            retwo = torchaudio.transforms.Resample(sr, newsr)(sig[1:,:]) # resample the 2nd channel\n",
    "            resig = torch.cat([resig, tetwo]) # merge both channels\n",
    "        \n",
    "        return ((resig, newsr))\n",
    "    \n",
    "    @staticmethod\n",
    "    def pad_trunc(aud, max_ms):\n",
    "        \"\"\"\n",
    "        Truncate the signal to a fixed length 'max_ms'\n",
    "        :param aud: tuple of signal and sample rate\n",
    "        :type aud: tuple\n",
    "        :param max_ms: max duration in milliseconds\n",
    "        :type max_ms: int\n",
    "        :return: returns a tuple of the signal and sample rate\n",
    "        \"\"\"\n",
    "        sig, sr = aud\n",
    "        num_rows, sig_len = sig.shape\n",
    "        max_len = sr//1000 * max_ms\n",
    "        \n",
    "        if (sig_len > max_len):\n",
    "            sig = sig[:,:max_len] # truncate to given length\n",
    "        \n",
    "        elif (sig_len < max_len):\n",
    "            pad_begin_len = random.randint(0, max_len - sig_len) # get the length to be added at the start of the signal\n",
    "            pad_end_len = max_len - sig_len - pad_begin_len # get the length to be added at the start of the signal\n",
    "            \n",
    "            # pad the signal with 0s\n",
    "            pad_begin = torch.zeros((num_rows, pad_begin_len))\n",
    "            pad_end = torch.zeros((num_rows, pad_end_len))\n",
    "            \n",
    "            sig = torch.cat((pad_begin, sig, pad_end), 1)\n",
    "        \n",
    "        return (sig, sr)\n",
    "    \n",
    "    @staticmethod\n",
    "    def time_shift(aud, shift_limit):\n",
    "        \"\"\"\n",
    "        Shifts the signal by some percentage to the left or right.\n",
    "        Audio which is \"out of bounds\" is wrapped around to the start of the signal.\n",
    "        :param aud: tuple of signal and sample rate\n",
    "        :type aud: tuple\n",
    "        :param shift_limit: shift_limit\n",
    "        :type shift_limit: int\n",
    "        :return: returns a tuple of the signal and sample rate\n",
    "        \"\"\"\n",
    "        sig, sr = aud\n",
    "        _, sig_len = sig.shape\n",
    "        shift_amt = int(random.random() * shift_limit * sig_len)\n",
    "        return (sig.roll(shift_amt), sr)\n",
    "    \n",
    "    @staticmethod\n",
    "    def spectro_gram(aud, n_mels=64, n_fft=1024, hop_len=None):\n",
    "        \"\"\"\n",
    "        Convert raw audio signal to a Mel Spectogram\n",
    "        :param aud: tuple of signal and sample rate\n",
    "        :type aud: tuple\n",
    "        :type n_mels: int\n",
    "        :type n_fft: int\n",
    "        :type hop_len: int\n",
    "        :return: MelSpectogram with Decibel scale\n",
    "        \"\"\"\n",
    "        sig, sr = aud\n",
    "        top_db = 80\n",
    "        \n",
    "        spec = transforms.MelSpectrogram(sr, n_fft=n_fft, hop_length=hop_len, n_mels=n_mels)(sig) # convert to Mel Spectrogram\n",
    "        \n",
    "        spec = transforms.AmplitudeToDB(top_db=top_db)(spec) # convert to Decibels\n",
    "        return (spec)\n",
    "\n",
    "    @staticmethod\n",
    "    def spectro_augment(spec, max_mask_pct=0.1, n_freq_masks=1, n_time_masks=1):\n",
    "        \"\"\"\n",
    "        Randomly augments the spectrogram to mask the frequency and time, similar as you would with the random skewing of images.\n",
    "        :param spec:\n",
    "        :type spec: PyTorch Spectogram\n",
    "        :type max_mas_pct: float\n",
    "        :type n_freq_masks: int\n",
    "        :type n_time_masks: int\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        _, n_mels, n_steps = spec.shape\n",
    "        mask_value = spec.mean()\n",
    "        aug_spec = spec\n",
    "        \n",
    "        freq_mask_param = max_mask_pct * n_mels\n",
    "        for _ in range(n_time_masks):\n",
    "            aug_spec = transforms.FrequencyMasking(freq_mask_param)(aug_spec, mask_value)\n",
    "            \n",
    "        time_mask_param = max_mask_pct * n_steps\n",
    "        for _ in range(n_time_masks):\n",
    "            aug_spec = transforms.TimeMasking(time_mask_param)(aug_spec, mask_value)\n",
    "        \n",
    "        return aug_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoundDS(Dataset):\n",
    "    \"\"\"\n",
    "    Custom Pytorch Dataset Class. This uses all the pre-processing from the AudioUtil object.\n",
    "    \"\"\"\n",
    "    def __init__(self, df, data_path):\n",
    "        self.df = df\n",
    "        self.data_path = str(data_path)\n",
    "        self.duration = 59000\n",
    "        self.sr = 44100\n",
    "        self.channel = 2\n",
    "        self.shift_pct = 0.4\n",
    "    \n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Native Python method to use the len() function.\n",
    "        :return: number of items in dataset\n",
    "        \"\"\"\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Native Python method to get an item by index\n",
    "        :param idx: index\n",
    "        :type idx: int\n",
    "        :return: item i from dataset as spectrogram and classID\n",
    "        \"\"\"\n",
    "        audio_file = self.data_path + self.df.loc[idx, 'relative_path']\n",
    "        class_id = self.df.loc[idx, 'classID']\n",
    "        \n",
    "        aud = AudioUtil.open(audio_file) # read audio file\n",
    "        reaud = AudioUtil.resample(aud, self.sr) # change sample rate\n",
    "        rechan = AudioUtil.rechannel(reaud, self.channel) # change mono to/from stereo\n",
    "        \n",
    "        dur_aud = AudioUtil.pad_trunc(rechan, self.duration) # adjust audio duration\n",
    "        shift_aud = AudioUtil.time_shift(dur_aud, self.shift_pct) # shift audio on the time\n",
    "        sgram = AudioUtil.spectro_gram(shift_aud, n_mels=64, n_fft=1024, hop_len=None) # make spectrogram\n",
    "        aug_sgram = AudioUtil.spectro_augment(sgram, max_mask_pct=0.1, n_freq_masks=2, n_time_masks=2) # mask frequency and time\n",
    "        \n",
    "        return aug_sgram, class_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# step 3: prepare batches of data\n",
    "\n",
    "After all the steps needed to create valid input for the model, we can use the custom Dataset to load the features and labels from our Pandas Dataframe from step 1. This is then randomly split in training and test set with an 80:20 split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "myds = SoundDS(df, download_path)\n",
    "\n",
    "num_items = len(myds)\n",
    "num_train = round(num_items * 0.8)\n",
    "num_val = num_items - num_train\n",
    "train_ds, test_ds = random_split(myds, [num_train, num_val])\n",
    "\n",
    "train_dl = torch.utils.data.DataLoader(train_ds, batch_size=108, shuffle=True) # total of classIDs * 2\n",
    "test_dl = torch.utils.data.DataLoader(test_ds, batch_size=108, shuffle=True) # total of classIDs * 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# step 4: create model\n",
    "\n",
    "The steps down below are quite similar to common standard image classification models. Since the dataset no longer consists of audio signals, but spectrogram images, we can use CNN (Convolutional Neural Networks) to process them. The different layers at the start are to \"break down\" the spectrogram into 256 inputs. Finally the nn.Linear uses the 256 inputs to predict for every 54 possible classes. (this is basically the Neural Networks class @Teun Salters, but this time we dont have to code it ourselves). The Sequential is basically a queue which will run all the different steps in a row.\n",
    "\n",
    "The training part will use the dataset multiple times with each time being called an 'Epoch'. With CrossEntropyLoss function to calculate the loss (predicted output vs expected output) and an optimizer to get the best parameters. The learning rate at the optimizer defines the \"step size\" it will take. Per epoch the code will predict for each \"sound\" and then use the loss function to decide how the weights have to be adjusted before starting another prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "from torch.nn import init\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioClassifier (nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        conv_layers = []\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(2, 8, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2))\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.bn1 = nn.BatchNorm2d(8)\n",
    "        init.kaiming_normal_(self.conv1.weight, a=0.1)\n",
    "        self.conv1.bias.data.zero_()\n",
    "        conv_layers += [self.conv1, self.relu1, self.bn1]\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(8, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.bn2 = nn.BatchNorm2d(16)\n",
    "        init.kaiming_normal_(self.conv2.weight, a=0.1)\n",
    "        self.conv2.bias.data.zero_()\n",
    "        conv_layers += [self.conv2, self.relu2, self.bn2]\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.bn3 = nn.BatchNorm2d(32)\n",
    "        init.kaiming_normal_(self.conv3.weight, a=0.1)\n",
    "        self.conv3.bias.data.zero_()\n",
    "        conv_layers += [self.conv3, self.relu3, self.bn3]\n",
    "        \n",
    "        self.conv4 = nn.Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
    "        self.relu4 = nn.ReLU()\n",
    "        self.bn4 = nn.BatchNorm2d(64)\n",
    "        init.kaiming_normal_(self.conv4.weight, a=0.1)\n",
    "        self.conv4.bias.data.zero_()\n",
    "        conv_layers += [self.conv4, self.relu4, self.bn4]\n",
    "        \n",
    "        self.conv5 = nn.Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
    "        self.relu5 = nn.ReLU()\n",
    "        self.bn5 = nn.BatchNorm2d(128)\n",
    "        init.kaiming_normal_(self.conv5.weight, a=0.1)\n",
    "        self.conv5.bias.data.zero_()\n",
    "        conv_layers += [self.conv5, self.relu5, self.bn5]\n",
    "        \n",
    "        self.conv6 = nn.Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
    "        self.relu6 = nn.ReLU()\n",
    "        self.bn6 = nn.BatchNorm2d(256)\n",
    "        init.kaiming_normal_(self.conv6.weight, a=0.1)\n",
    "        self.conv6.bias.data.zero_()\n",
    "        conv_layers += [self.conv6, self.relu6, self.bn6]\n",
    "        \n",
    "        self.ap = nn.AdaptiveAvgPool2d(output_size=1)\n",
    "        self.lin = nn.Linear(in_features=256, out_features=54)\n",
    "        \n",
    "        self.conv = nn.Sequential(*conv_layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        \n",
    "        x = self.ap(x)\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        \n",
    "        x = self.lin(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myModel = AudioClassifier()\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\") # this tries to use the GPU if possible. RIP Linux users with nvidia gpu\n",
    "myModel = myModel.to(device)\n",
    "next(myModel.parameters()).device # print if it's on cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(model, train_dl, num_epochs):\n",
    "    criterion = nn.CrossEntropyLoss() # loss function, which calculates the distance between current output and expected output\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001) # find the best parameters with a learning rate (step size) of 0.001\n",
    "    scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer,\n",
    "                                                    max_lr=0.001,\n",
    "                                                    steps_per_epoch=int(len(train_dl)),\n",
    "                                                    epochs=num_epochs,\n",
    "                                                    anneal_strategy='linear') # tie them nicely together\n",
    "    \n",
    "    acc_list = [] # to create a fancy plot later on\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        correct_prediction = 0\n",
    "        total_prediction = 0\n",
    "        \n",
    "        for i, data in enumerate(train_dl):\n",
    "            inputs, labels = data[0].to(device), data[1].to(device) # input features and target labels\n",
    "            \n",
    "            inputs_m, inputs_s = inputs.mean(), inputs.std()\n",
    "            inputs = (inputs - inputs_m) / inputs_s # normalize input features\n",
    "            \n",
    "            optimizer.zero_grad() # reset optimizer\n",
    "            \n",
    "            outputs = model(inputs) # predict output\n",
    "            loss = criterion(outputs, labels) # calculate loss\n",
    "            loss.backward() # count changes in weights\n",
    "            optimizer.step() # optimize\n",
    "            scheduler.step() # run 1 schedule step\n",
    "            \n",
    "            running_loss += loss.item() # stats for Loss and Accuracy\n",
    "            \n",
    "            _, prediction = torch.max(outputs, 1) # get the class with the highest prediction\n",
    "            \n",
    "            correct_prediction += (prediction == labels).sum().item() \n",
    "            total_prediction+= prediction.shape[0]\n",
    "        \n",
    "        # print stats\n",
    "        num_batches = len(train_dl)\n",
    "        avg_loss = running_loss / num_batches\n",
    "        acc = correct_prediction / total_prediction\n",
    "        acc_list.append(acc)\n",
    "        print(f'Epoch: {epoch + 1}, Loss: {avg_loss:.2f}, Accuracy: {acc:.2f}')\n",
    "    \n",
    "    print('Finished Training')\n",
    "\n",
    "num_epochs = 50\n",
    "training(myModel, train_dl, num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# step 5: validating the model\n",
    "\n",
    "The last step is to use the test data we split in step 4 to validate the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interference (model, test_dl):\n",
    "    correct_pred = 0\n",
    "    total_pred = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data in test_dl:\n",
    "            inputs, labels = data[0].to(device), data[1].to(device) # input features and target labels\n",
    "            \n",
    "            inputs_m, inputs_s = inputs.mean(), inputs.std()\n",
    "            inputs = (inputs - inputs_m) / inputs_s # normalize input features\n",
    "            \n",
    "            outputs = model(inputs) # predict output\n",
    "            \n",
    "            _, prediction = torch.max(outputs, 1) # get the class with the highest prediction\n",
    "            correct_pred += (prediction == labels).sum.item()\n",
    "            total_pred += prediction.shape[0]\n",
    "            \n",
    "    acc = correct_pred / total_pred\n",
    "    print(f'Accuracy: {acc:.2f}, Total items: {total_pred}')\n",
    "\n",
    "interference(myModel, test_dl)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
